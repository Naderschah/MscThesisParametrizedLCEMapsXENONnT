{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verification notebook\n",
    "\n",
    "Here every component implemented in tensorflow, jax and numpy is cross verified \n",
    "\n",
    "First that they all produce the same output then some timing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Verify Jax vs Np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "# Server vs Local \n",
    "if '/Code' in os.getcwd():\n",
    "    os.chdir('/Code/ActualThesisWork')\n",
    "    %pip install monotonic-nn==0.3.5 # Just cloned it into a dir on server\n",
    "    %pip install tensorflow_probability==0.23 # Comes with 04.03.24\n",
    "else:\n",
    "    os.chdir('/scratch/midway3/fsemler')\n",
    "\n",
    "sys.path.append(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import layer_reimplementation_jax as ld_jax\n",
    "import layer_reimplementation_np as ld_np\n",
    "import layer_definitions as ld\n",
    "# To compile the numpy code \n",
    "import numba\n",
    "\n",
    "# to generate random numbers\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Some things we will need for testing\n",
    "\n",
    "i0s = #TODO\n",
    "pmt_pos_top = # TODO \n",
    "r_tpc = 66.4\n",
    "\n",
    "n_events = 1_000\n",
    "n_pmts = len(pmt_pos_top)\n",
    "\n",
    "def points_in_circle(n_points, seed = 0):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    # sample angles uniformly\n",
    "    theta = rng.random(n_points) * 2 * np.pi            # [0, 2π)\n",
    "    # sample radii with sqrt for area density\n",
    "    r     = tpc_r * np.sqrt(rng.random(n_points))       # [0, R]\n",
    "    # convert to Cartesian\n",
    "    x = r * np.cos(theta)\n",
    "    y = r * np.sin(theta)\n",
    "    return np.stack([x, y], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Some utility functions\n",
    "\n",
    "def compare_arrays(a, b, rtol: float = 1e-6) -> None:\n",
    "    \"\"\"\n",
    "    Compare two array-like inputs (NumPy, JAX, or TensorFlow) for near-equality.\n",
    "    Prints whether all elements are close within the given relative tolerance.\n",
    "    If not, prints a table of absolute difference statistics.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    a, b : array-like or tensor\n",
    "        Inputs to compare. Can be numpy.ndarray, JAX arrays, or TensorFlow tensors.\n",
    "    rtol : float\n",
    "        Relative tolerance for np.allclose.\n",
    "    \"\"\"\n",
    "    def to_numpy(x):\n",
    "        # TensorFlow tensor\n",
    "        if _has_tf and isinstance(x, tf.Tensor):\n",
    "            return x.numpy()\n",
    "        # JAX array\n",
    "        if _has_jax and isinstance(x, jax.Array):\n",
    "            return np.array(x)\n",
    "        # NumPy or other array-like\n",
    "        return np.array(x)\n",
    "\n",
    "    a_np = to_numpy(a).astype(np.float64)\n",
    "    b_np = to_numpy(b).astype(np.float64)\n",
    "\n",
    "    if a_np.shape != b_np.shape:\n",
    "        print(f\"Shape mismatch: {a_np.shape} vs {b_np.shape}\")\n",
    "        return\n",
    "\n",
    "    if np.allclose(a_np, b_np, rtol=rtol):\n",
    "        print(f\"All elements close within rtol={rtol}\")\n",
    "    else:\n",
    "        diff = np.abs(a_np - b_np)\n",
    "        # Single-line table\n",
    "        metrics = [\"max diff\", \"min diff\", \"mean diff\", \"std diff\"]\n",
    "        values = [f\"{diff.max():.6g}\", f\"{diff.min():.6g}\", f\"{diff.mean():.6g}\", f\"{diff.std():.6g}\"]\n",
    "        # Print headers\n",
    "        print(\" | \".join(metrics))\n",
    "        # Print values\n",
    "        print(\" | \".join(values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "jnp_func_to_test = (\n",
    "    # All inputs are tested with the first func\n",
    "    \"get_input_functions\",\n",
    "    \"make_normalization_layer\",\n",
    "    \"make_dense_layer\",\n",
    "    \"make_mono_activations\",\n",
    "    \"make_I0_layer\",\n",
    "    \"make_radial_lce_layer\", \n",
    "    # All below done at once through last\n",
    "    \"make_lut_table_with_std\",\n",
    "    \"make_lut_table_fixed_std\",\n",
    "    \"make_exact_lr\",\n",
    "    \"make_lut_trainable_std_lr\",\n",
    "    \"make_lut_fixed_std_lr\",\n",
    "    \"make_likelihood_fn\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Returns 4 functions - all the input functions\n",
    "j_out  = ld_jax.get_input_functions(pmt_pos_top)\n",
    "np_out = ld_np.get_input_functions(pmt_pos_top)\n",
    "nb_out = numba.njit(ld_np.get_input_functions(pmt_pos_top))\n",
    "\n",
    "pos = points_in_circle(n_events)\n",
    "\n",
    "j_out   = [i(pos) for i in j_out]\n",
    "np_out  = [i(pos) for i in np_out]\n",
    "nb_out  = [i(pos) for i in nb_out]\n",
    "\n",
    "del pos\n",
    "\n",
    "for i in range(len(j_out)):\n",
    "    print(\"jax v np\")\n",
    "    compare_arrays(j_out[i], np_out[i])\n",
    "    print(\"np v nb\")\n",
    "    compare_arrays(np_out[i], nb_out[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "random_input = scipt.stats.poisson(mu=1000).rvs(size=(512, n_pmts))\n",
    "\n",
    "j_out  = ld_jax.make_normalization_layer()(random_input)\n",
    "np_out = ld_np.make_normalization_layer()(random_input)\n",
    "nb_out = numba.njit(ld_np.make_normalization_layer())(random_input)\n",
    "\n",
    "del random_input\n",
    "\n",
    "print(\"jax v np\")\n",
    "compare_arrays(j_out, np_out)\n",
    "print(\"np v nb\")\n",
    "compare_arrays(np_out, nb_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 246\n",
    "rng        = np.random.default_rng(0)\n",
    "for base_act in ['tanh', 'exponential', 'relu', 'sigmoid']:\n",
    "    # Random layer dimensions\n",
    "    input_dim = rng.integers(1, 33)\n",
    "    units     = rng.integers(1, 33)\n",
    "\n",
    "    # Random kernel & bias\n",
    "    kernel_np = rng.standard_normal((input_dim, units), dtype=np.float32)\n",
    "    bias_np   = rng.standard_normal((units,),           dtype=np.float32)\n",
    "\n",
    "    # Build activation functions\n",
    "    jax_act = ld_jax.make_mono_activations(base_act)\n",
    "    np_act  = ld_np.make_mono_activations(base_act)\n",
    "\n",
    "    # Instantiate layers\n",
    "    jax_layer = ld_jax.make_dense_layer(jnp.array(kernel_np),\n",
    "                                        jnp.array(bias_np),\n",
    "                                        jax_act)\n",
    "    np_layer  = ld_np.make_dense_layer(kernel_np,\n",
    "                                       bias_np,\n",
    "                                       np_act)\n",
    "\n",
    "    # Numba‐compile the NumPy layer\n",
    "    nb_layer  = numba.njit(np_layer)\n",
    "\n",
    "    # Generate random input\n",
    "    X_np = rng.standard_normal((batch_size, n_pmts, input_dim), dtype=np.float32)\n",
    "    X_j  = jnp.array(X_np)\n",
    "\n",
    "    # Compute outputs\n",
    "    y_jax = np.array(jax_layer(X_j))    # JAX → NumPy\n",
    "    y_np  =  np_layer(X_np)\n",
    "    y_nb  =  nb_layer(X_np)\n",
    "\n",
    "    # Compare\n",
    "    print(f\"\\n=== base_act={base_act!r}, input_dim={input_dim}, units={units} ===\")\n",
    "    print(\"jax vs np: \", end=\"\"); compare_arrays(y_jax, y_np)\n",
    "    print(\"np vs nb:  \", end=\"\"); compare_arrays(y_np,  y_nb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "random_input = scipt.stats.poisson(mu=1000).rvs(size=(512, n_pmts))\n",
    "i0s = np.random.rand(n_pmts)\n",
    "\n",
    "j_out  = ld_jax.make_I0_layer(i0s)(random_input)\n",
    "np_out = ld_np.make_I0_layer(i0s)(random_input)\n",
    "nb_out = numba.njit(ld_np.make_I0_layer(i0s))(random_input)\n",
    "\n",
    "del random_input\n",
    "\n",
    "print(\"jax v np\")\n",
    "compare_arrays(j_out, np_out)\n",
    "print(\"np v nb\")\n",
    "compare_arrays(np_out, nb_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Pretrained guess -> easiest\n",
    "params = [1.6266745e+00,  9.4918861e+00, -4.2176653e-05,  7.7804564e-03,]\n",
    "\n",
    "random_input = tpc_r * np.sqrt(rng.random(n_points))\n",
    "\n",
    "j_out  = ld_jax.make_radial_lce_layer(params)(random_input)\n",
    "np_out = ld_np.make_radial_lce_layer(params)(random_input)\n",
    "nb_out = numba.njit(ld_np.make_radial_lce_layer(params))(random_input)\n",
    "\n",
    "del random_input\n",
    "\n",
    "print(\"jax v np\")\n",
    "compare_arrays(j_out, np_out)\n",
    "print(\"np v nb\")\n",
    "compare_arrays(np_out, nb_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# And the likelihood functions \n",
    "\n",
    "# ---- 1) shared parameters ----\n",
    "P = 16                     # small PMT count for speed\n",
    "switching_signal = 10.0\n",
    "p_dpe = 0.2\n",
    "m, z = 2, 2                # small subdivisions for test\n",
    "nan_safe_value = 1e5\n",
    "\n",
    "# 1a) build domains in NumPy\n",
    "n_pe_domain_np = np.arange(\n",
    "    0.0,\n",
    "    switching_signal + 5 * np.sqrt(switching_signal) + 2\n",
    ")\n",
    "n_ph_domain_np = np.arange(\n",
    "    0.0,\n",
    "    switching_signal / (1 + p_dpe)\n",
    "    + 5 * np.sqrt(switching_signal / (1 + p_dpe))\n",
    "    + 2\n",
    ")\n",
    "x_domain_np     = np.linspace(0, switching_signal, 40*m + 1)\n",
    "sigma_domain_np = np.linspace(0.05, 1.0, z)\n",
    "\n",
    "# 1b) corresponding JAX arrays\n",
    "n_pe_domain_j = jnp.array(n_pe_domain_np, dtype=jnp.float32)\n",
    "n_ph_domain_j = jnp.array(n_ph_domain_np, dtype=jnp.float32)\n",
    "x_domain_j    = jnp.array(x_domain_np,    dtype=jnp.float32)\n",
    "sigma_domain_j = jnp.array(sigma_domain_np, dtype=jnp.float32)\n",
    "\n",
    "# ---- 2) Test LUT‐table generators ----\n",
    "# 2a) variable‐std LUT\n",
    "jax_lut_var = ld_jax.make_lut_table_with_std(\n",
    "    n_pe_domain_j, n_ph_domain_j,\n",
    "    x_domain_j, sigma_domain_j,\n",
    "    switching_signal, p_dpe\n",
    ")\n",
    "np_lut_var  = ld_np.make_lut_table_with_std(\n",
    "    n_pe_domain_np, n_ph_domain_np,\n",
    "    x_domain_np, sigma_domain_np,\n",
    "    switching_signal, p_dpe\n",
    ")\n",
    "nb_lut_var_fn = numba.njit(ld_np.make_lut_table_with_std)\n",
    "nb_lut_var    = nb_lut_var_fn(\n",
    "    n_pe_domain_np, n_ph_domain_np,\n",
    "    x_domain_np, sigma_domain_np,\n",
    "    switching_signal, p_dpe\n",
    ")\n",
    "print(\"LUT_var JAX vs NP:\");    compare_arrays(np.array(jax_lut_var),  np_lut_var)\n",
    "print(\"LUT_var NP vs NB:\");    compare_arrays(                  np_lut_var, nb_lut_var)\n",
    "\n",
    "\n",
    "# 2b) fixed‐std LUT\n",
    "# here we need stds for sigma‐domain\n",
    "stds_np = np.ones((P,),dtype=np.float32)*0.5\n",
    "stds_j  = jnp.array(stds_np)\n",
    "jax_lut_fix = ld_jax.make_lut_table_fixed_std(\n",
    "    n_pe_domain_j, n_ph_domain_j,\n",
    "    x_domain_j, stds_j,\n",
    "    switching_signal, p_dpe\n",
    ")\n",
    "np_lut_fix  = ld_np.make_lut_table_fixed_std(\n",
    "    n_pe_domain_np, n_ph_domain_np,\n",
    "    x_domain_np, stds_np,\n",
    "    switching_signal, p_dpe\n",
    ")\n",
    "nb_lut_fix_fn = numba.njit(ld_np.make_lut_table_fixed_std)\n",
    "nb_lut_fix    = nb_lut_fix_fn(\n",
    "    n_pe_domain_np, n_ph_domain_np,\n",
    "    x_domain_np, stds_np,\n",
    "    switching_signal, p_dpe\n",
    ")\n",
    "print(\"LUT_fix JAX vs NP:\");  compare_arrays(np.array(jax_lut_fix),  np_lut_fix)\n",
    "print(\"LUT_fix NP vs NB:\");  compare_arrays(                 np_lut_fix, nb_lut_fix)\n",
    "\n",
    "# ---- 3) Test exact‐LR generator ----\n",
    "jax_exact = ld_jax.make_exact_lr(\n",
    "    n_pe_domain_j, n_ph_domain_j, p_dpe,\n",
    "    switching_signal, nan_safe=True, nan_safe_value=nan_safe_value\n",
    ")\n",
    "np_exact  = ld_np.make_exact_lr(\n",
    "    n_pe_domain_np, n_ph_domain_np, p_dpe,\n",
    "    switching_signal, nan_safe=True, nan_safe_value=nan_safe_value\n",
    ")\n",
    "nb_exact_fn = numba.njit(np_exact)\n",
    "# sample small test\n",
    "B = 8\n",
    "x_test = np.random.uniform(0, switching_signal, size=(B,))\n",
    "mu_test = np.random.uniform(0, switching_signal, size=(B,))\n",
    "std_test = np.ones((B,))*0.5\n",
    "# run\n",
    "j_ex = np.array(jax_exact(jnp.array(x_test), jnp.array(mu_test), jnp.array(std_test)))\n",
    "n_ex =            np_exact(    x_test,             mu_test,             std_test)\n",
    "nb_ex =           nb_exact_fn(x_test,             mu_test,             std_test)\n",
    "print(\"exactLR JAX vs NP:\");    compare_arrays(j_ex,  n_ex)\n",
    "print(\"exactLR NP vs NB:\");    compare_arrays(n_ex, nb_ex)\n",
    "\n",
    "jax_train = ld_jax.make_lut_trainable_std_lr(\n",
    "    jax_lut_var, x_domain_j, sigma_domain_j,\n",
    "    switching_signal, lambda x: x/1.2,\n",
    "    return_ratio=False, nan_safe=True, nan_safe_value=nan_safe_value\n",
    ")\n",
    "np_train = ld_np.make_lut_trainable_std_lr(\n",
    "    np_lut_var, x_domain_np, sigma_domain_np,\n",
    "    switching_signal, lambda x: x/1.2,\n",
    "    return_ratio=False, nan_safe=True, nan_safe_value=nan_safe_value\n",
    ")\n",
    "nb_train_fn = numba.njit(np_train)\n",
    "# sample pred/obs/std: shape (B,P)\n",
    "pred = np.random.uniform(0, switching_signal, size=(B,P))\n",
    "obs  = np.random.uniform(0, switching_signal, size=(B,P))\n",
    "std_  = np.tile(stds_np[None,:], (B,1))\n",
    "# run\n",
    "j_tr = np.array(jax_train(jnp.array(pred), jnp.array(obs), jnp.array(std_)))\n",
    "n_tr =            np_train(     pred,                obs,                std_)\n",
    "nb_tr =           nb_train_fn(pred,                obs,                std_)\n",
    "print(\"trainLR JAX vs NP:\");    compare_arrays(j_tr,  n_tr)\n",
    "print(\"trainLR NP vs NB:\");    compare_arrays(n_tr, nb_tr)\n",
    "\n",
    "\n",
    "modes = [\"exact\", \"LUT_trainable_std\", \"LUT_fixed_std\"]\n",
    "flags = list(product(modes, (False, True), (False, True)))\n",
    "\n",
    "for mode, return_ratio, nan_safe in flags:\n",
    "    print(f\"\\n=== mode={mode}, return_ratio={return_ratio}, nan_safe={nan_safe} ===\")\n",
    "\n",
    "    # build kwargs for JAX and NumPy\n",
    "    common = dict(\n",
    "        mode=mode,\n",
    "        return_ratio=return_ratio,\n",
    "        nan_safe=nan_safe,\n",
    "        nan_safe_value=nan_safe_value,\n",
    "        switching_signal=switching_signal,\n",
    "        mle_estimator=lambda x: x/1.2,\n",
    "        std=jnp.ones((P,), jnp.float32)*0.5,\n",
    "        p_dpe=p_dpe\n",
    "    )\n",
    "    if mode == \"LUT_trainable_std\":\n",
    "        jargs = {**common, \"lut_table\": jax_lut_var,\n",
    "                       \"x_domain\": x_domain_j,\n",
    "                       \"sigma_domain\": sigma_domain_j}\n",
    "        npargs= {**common, \"lut_table\": np_lut_var,\n",
    "                       \"x_domain\": x_domain_np,\n",
    "                       \"sigma_domain\": sigma_domain_np}\n",
    "    elif mode == \"LUT_fixed_std\":\n",
    "        jargs = {**common, \"lut_table\": jax_lut_fix,\n",
    "                       \"x_domain\": x_domain_j}\n",
    "        npargs= {**common, \"lut_table\": np_lut_fix,\n",
    "                       \"x_domain\": x_domain_np}\n",
    "    else:  # exact\n",
    "        jargs = common\n",
    "        npargs= common\n",
    "\n",
    "    # instantiate\n",
    "    jfn  = ld_jax.make_likelihood_fn(**jargs)\n",
    "    npfn = ld_np.make_likelihood_fn(**npargs)\n",
    "    nbfn = numba.njit(npfn)\n",
    "\n",
    "    # run\n",
    "    j_out  = np.array(jfn(jnp.array(pred), jnp.array(obs)))\n",
    "    np_out =  npfn(pred, obs)\n",
    "    nb_out =  nbfn(pred, obs)\n",
    "\n",
    "    print(\"jax vs np: \", end=\"\"); compare_arrays(j_out,  np_out)\n",
    "    print(\"np vs nb: \", end=\"\"); compare_arrays(np_out, nb_out)\n",
    "\n",
    "    # clean up big locals\n",
    "    del jfn, npfn, nbfn, j_out, np_out, nb_out\n",
    "\n",
    "# once you’re done with both LUT tables, delete them too:\n",
    "del jax_lut_var, np_lut_var, jax_lut_fix, np_lut_fix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Verify Tf vs Jax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#TODO\n",
    "\n",
    "Probably easiest if i wait till the model is trained generate two full models and verify the output is the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Now Timing\n",
    "\n",
    "We only time the full model, no need to do partial "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "weights_path = #TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def gen_tf_model(pmt_pos_top, weights_path,\n",
    "    include_wall=True, include_perp=True,\n",
    "    include_anode=True, multiplication_layers=False,\n",
    "    radialLCE=False,\n",
    "    ):\n",
    "    # FIXME: \n",
    "    raise Exception(\"Not Implemented\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "jax_model_fn = gen_jax_model(\n",
    "    pmt_pos_top, weights_path,\n",
    "    include_wall=True, include_perp=True,\n",
    "    include_anode=True, multiplication_layers=False,\n",
    "    radialLCE=False,\n",
    ")\n",
    "np_model_fn = gen_np_model(\n",
    "    pmt_pos_top, weights_path,\n",
    "    include_wall=True, include_perp=True,\n",
    "    include_anode=True, multiplication_layers=False,\n",
    "    radialLCE=False,\n",
    ")\n",
    "tf_model    = gen_tf_model(\n",
    "    pmt_pos_top, weights_path,\n",
    "    include_wall=True, include_perp=True,\n",
    "    include_anode=True, multiplication_layers=False,\n",
    "    radialLCE=False,\n",
    ")\n",
    "\n",
    "# 2) Compile / jit / njit\n",
    "jax_compiled = jax.jit(jax_model_fn)\n",
    "nb_model     = numba.njit(np_model_fn)\n",
    "\n",
    "# 3) Warm-up and trigger compilation\n",
    "B_warm = 8\n",
    "xy_warm  = sample_points_in_circle(B_warm, tpc_r=66.4, seed=0)\n",
    "obs_warm = stats.poisson(mu=1000).rvs(size=(B_warm, pmt_pos_top.shape[0]))\n",
    "_ = jax_compiled(jnp.array(xy_warm), jnp.array(obs_warm)).block_until_ready()\n",
    "_ = np_model_fn(xy_warm, obs_warm)\n",
    "_ = nb_model(xy_warm, obs_warm)\n",
    "_ = tf_model(xy_warm, obs_warm)  # eager TF call\n",
    "del xy_warm, obs_warm\n",
    "\n",
    "# 4) Helper timers\n",
    "def time_trials(fn, args, n_trials=10, is_jax=False, is_tf=False):\n",
    "    times = []\n",
    "    for _ in range(n_trials):\n",
    "        t0 = time.perf_counter()\n",
    "        out = fn(*args)\n",
    "        if is_jax:\n",
    "            out.block_until_ready()\n",
    "        elif is_tf:\n",
    "            # ensure TensorFlow finishes\n",
    "            if isinstance(out, tf.Tensor): out.numpy()\n",
    "        t1 = time.perf_counter()\n",
    "        times.append(t1 - t0)\n",
    "    arr = np.array(times, dtype=np.float64)\n",
    "    return arr.mean(), arr.std()\n",
    "\n",
    "# 5) Benchmark over batch sizes\n",
    "print(\"#Events |    JAX (ms)    |   NumPy (ms)  |  Numba (ms)  |   TF (ms)\")\n",
    "for n_events in [100, 1_000, 5_000, 10_000]:\n",
    "    xy = sample_points_in_circle(n_events, tpc_r=66.4)\n",
    "    obs= stats.poisson(mu=1000).rvs(size=(n_events, pmt_pos_top.shape[0]))\n",
    "\n",
    "    args_j = (jnp.array(xy), jnp.array(obs))\n",
    "    args_p = (xy, obs)\n",
    "    args_t = (xy, obs)\n",
    "\n",
    "    j_me, j_st = time_trials(jax_compiled, args_j, n_trials=10, is_jax=True)\n",
    "    p_me, p_st = time_trials(np_model_fn,  args_p, n_trials=10)\n",
    "    n_me, n_st = time_trials(nb_model,     args_p, n_trials=10)\n",
    "    t_me, t_st = time_trials(tf_model.__call__, args_t, n_trials=10, is_tf=True)\n",
    "\n",
    "    print(f\"{n_events:7d} | \"\n",
    "          f\"{j_me*1e3:7.2f}±{j_st*1e3:5.2f} | \"\n",
    "          f\"{p_me*1e3:7.2f}±{p_st*1e3:5.2f} | \"\n",
    "          f\"{n_me*1e3:7.2f}±{n_st*1e3:5.2f} | \"\n",
    "          f\"{t_me*1e3:7.2f}±{t_st*1e3:5.2f}\")\n",
    "\n",
    "    # also compare outputs once for equivalence\n",
    "    j_out = np.array(jax_compiled(jnp.array(xy), jnp.array(obs)))\n",
    "    np_out= np_model_fn(xy, obs)\n",
    "    nb_out= nb_model(xy, obs)\n",
    "    tf_out= tf_model(xy, obs).numpy()\n",
    "\n",
    "    print(\"  compare JAX vs NP: \", end=\"\"); compare_arrays(j_out,  np_out)\n",
    "    print(\"  compare NP vs NB:  \", end=\"\"); compare_arrays(np_out, nb_out)\n",
    "    print(\"  compare NP vs TF:  \", end=\"\"); compare_arrays(np_out, tf_out)\n",
    "\n",
    "    del xy, obs, j_out, np_out, nb_out, tf_out\n",
    "\n",
    "# 6) cleanup\n",
    "del jax_model_fn, np_model_fn, tf_model, jax_compiled, nb_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Compute JAX Flops From XLA dump\n",
    "\n",
    "\n",
    "TODO below is a basic script to compare them amongst one another check all works then do the following:\n",
    "\n",
    "- Comparison baseline bs = 1024, n_pmts = n_alive \n",
    "\n",
    "- Within each category vary, plot and print all options FLOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from layer_reimplementation_jax import gen_jax_model\n",
    "\n",
    "BATCH = 2\n",
    "xy_dummy  = jnp.zeros((BATCH, 2),  dtype=jnp.float32)\n",
    "obs_dummy = jnp.zeros((BATCH, pmt_pos_top.shape[0]), dtype=jnp.float32)\n",
    "\n",
    "# Define the variants you care about\n",
    "variants = [\n",
    "    (\"direct_only\", dict(include_wall=False, include_perp=False, include_anode=False,\n",
    "                         multiplication_layers=False, radialLCE=False)),\n",
    "    (\"with_wall\",   dict(include_wall=True,  include_perp=False, include_anode=False,\n",
    "                         multiplication_layers=False, radialLCE=False)),\n",
    "    (\"with_perp\",   dict(include_wall=False, include_perp=True,  include_anode=False,\n",
    "                         multiplication_layers=False, radialLCE=False)),\n",
    "    (\"with_anode\",  dict(include_wall=False, include_perp=False, include_anode=True,\n",
    "                         multiplication_layers=False, radialLCE=False)),\n",
    "    (\"all_add\",     dict(include_wall=True,  include_perp=True,  include_anode=True,\n",
    "                         multiplication_layers=False, radialLCE=False)),\n",
    "    (\"all_mul\",     dict(include_wall=True,  include_perp=True,  include_anode=True,\n",
    "                         multiplication_layers=True,  radialLCE=False)),\n",
    "    (\"radialLCE\",   dict(include_wall=False, include_perp=False, include_anode=False,\n",
    "                         multiplication_layers=False, radialLCE=True)),\n",
    "]\n",
    "\n",
    "for name, cfg in variants:\n",
    "    print(f\"\\n=== Tracing variant: {name} ===\")\n",
    "    # 1) build the JAX model function\n",
    "    model_fn = gen_jax_model(\n",
    "        pmt_pos_top, weights_path,\n",
    "        **cfg\n",
    "    )\n",
    "\n",
    "    # 2) get its XLA computation\n",
    "    xla_comp = jax.xla_computation(model_fn)(xy_dummy, obs_dummy)\n",
    "\n",
    "    # 3a) grab the human-readable HLO text\n",
    "    hlo_text = xla_comp.as_hlo_text()\n",
    "\n",
    "    # 3b) grab the raw HLO proto\n",
    "    hlo_proto = xla_comp.as_hlo_proto()\n",
    "\n",
    "    # 4) either inspect in Python...\n",
    "    print(\"HLO snippet:\")\n",
    "    for line in hlo_text.splitlines()[0:10]:\n",
    "        print(\"  \", line)\n",
    "    # ...or save to files for later profiling:\n",
    "    with open(f\"hlo_{name}.txt\", \"w\") as ftxt:\n",
    "        ftxt.write(hlo_text)\n",
    "    with open(f\"hlo_{name}.pb\", \"wb\") as fbin:\n",
    "        fbin.write(hlo_proto.SerializeToString())\n",
    "\n",
    "    # clean up before next variant\n",
    "    del model_fn, xla_comp, hlo_text, hlo_proto\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Investigate model architectures to broadcast events from andoe to gate\n",
    "\n",
    "Strategy:\n",
    "- Could exploit expected homogenity -> But until what radius -> The edges are massively underpopulated for expectation -> Plot r^2 vs event dist normalized to relative observed area per bin, take cutoff when distribution drops -> Poor edge performance\n",
    "- Use Background edge events? \n",
    "\n",
    "Also need to find dataset where i can resolve the anode mesh \n",
    "\n",
    "Could also try using the naive anode model\n",
    "\n",
    "Or use Ripley Statistic for homogeneity train NN inspect functional form at different location in tpc, most important will be walls, and around anode/gate perp and mesh and find some function that can fit all three of these\n",
    "\n",
    "Ripley functions:\n",
    "- K : The sample-based estimate is defined as:\n",
    "$$\\hat{K}(t) = \\lambda^{-1}\\sum_{i\\neq j}\\frac{I(d_{ij}<t)}{n}$$\n",
    "where d is the euclidean distance, t is the search radius, $\\lambda$ is the average point density (over the entire set) and I is the indicator function (1 if true, 0 if false : Ie its a boolean)\n",
    "\n",
    "If points are homogeneous we expect that $\\hat{K}(t)=\\pi t^2$ for 2D data. \n",
    "\n",
    "- L : The variance stabilized Ripley K function, the sample based function is:\n",
    "$$\\hat{L}(t) = \\left(\\frac{\\hat{K}(t)}{\\pi}\\right)^{1/2}$$\n",
    "\n",
    "For a homogeneous distribution we expect that $\\hat{L}(t) = t$ with variance in t being approximately constant (ie the same for all t)\n",
    "\n",
    "\n",
    "\n",
    "Also note: Fucking piece of shit tensorflow cant cast values so I cant use Truth values for mathematics without breaking differentiability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-29 12:07:16,811 - admix - WARNING - Initializing utilix DB failed. You cannot do database operations\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DB initialization failed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-29 12:07:17.591095: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-11-29 12:07:17.592843: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-11-29 12:07:17.613350: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-11-29 12:07:17.613373: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-11-29 12:07:17.614100: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-29 12:07:17.617770: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-11-29 12:07:17.618090: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-11-29 12:07:18.032949: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import straxen\n",
    "\n",
    "import sys, os\n",
    "import h5py\n",
    "\n",
    "import tensorflow\n",
    "from tensorflow import keras\n",
    "import numpy as jnp\n",
    "\n",
    "sys.path.append(os.path.join(os.getcwd(), 'ActualThesisWork'))\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from functions import *\n",
    "\n",
    "\n",
    "tensorflow.keras.backend.set_floatx('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Basic variables\n",
    "dead_pmts = jnp.array([ 28, 108, 121, 144, 156, 164, 177])\n",
    "not_dead_pmts = jnp.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 157, 158, 159, 160, 161, 162, 163, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252])\n",
    "pmt_radius = straxen.tpc_pmt_radius# 3  * 2.54 / 2 \n",
    "tpc_radius = 66.4\n",
    "max_z = straxen.tpc_z\n",
    "I0_init_from_ULCE = 0.1 # This is never act used\n",
    "pmt_pos = straxen.pmt_positions()\n",
    "pmt_pos_top = pmt_pos[pmt_pos.array == \"top\"].to_numpy()\n",
    "n_pmts = pmt_pos_top.shape[0]\n",
    "\n",
    "lambda_ = jnp.pi * tpc_radius**2\n",
    "\n",
    "def RipleyK(dat, t):\n",
    "    pairwise_dist = jnp.linalg.norm(dat[:, jnp.newaxis, :] - dat[np.newaxis, :, :], axis=2)\n",
    "    indicator_func = (pairwise_dist < t)\n",
    "    jnp.fill_diagonal(indicator_func, 0) # i \\neq j \n",
    "    return (1/(lambda_ * dat.shape[0])) * jnp.sum(indicator_func)\n",
    "\n",
    "def RipleyL(dat, tpc_radius, min_t, samples=100):\n",
    "    ts = jnp.linspace(min_t, tpc_radius, samples)\n",
    "    Ls = jnp.zeros_like(ts)\n",
    "    for i in range(len(Ls)):\n",
    "        Ls[i] = jnp.sqrt(RipleyK(dat, ts[i])/np.pi)\n",
    "    return Ls, ts\n",
    "\n",
    "# And tensorflow version\n",
    "def RipleyK_vectorized(dat, ts):\n",
    "    \"\"\"\n",
    "    Compute Ripley's K for multiple t values in a vectorized way.\n",
    "    \"\"\"\n",
    "    # Compute pairwise distances\n",
    "    dat_expanded_1 = tensorflow.expand_dims(dat, axis=1)  # Shape: (N, 1, D)\n",
    "    dat_expanded_2 = tensorflow.expand_dims(dat, axis=0)  # Shape: (1, N, D)\n",
    "    pairwise_dist = tensorflow.norm(dat_expanded_1 - dat_expanded_2, axis=2)  # Shape: (N, N)\n",
    "    \n",
    "    # Expand ts for comparison (N, N, samples)\n",
    "    ts_expanded = tensorflow.reshape(ts, (1, 1, -1))\n",
    "    \n",
    "    # Compute indicator function for each t\n",
    "    #indicator_func = tensorflow.cast(pairwise_dist[:, :, tensorflow.newaxis] < ts_expanded, tensorflow.float32)\n",
    "    \n",
    "    # Remove diagonal (i != j)\n",
    "    # Mask diagonal elements for all samples\n",
    "    #diag_mask = tensorflow.eye(tensorflow.shape(pairwise_dist)[0], dtype=tensorflow.float32)  # Shape: (N, N)\n",
    "    #diag_mask = tensorflow.expand_dims(diag_mask, axis=-1)  # Shape: (N, N, 1)\n",
    "    #indicator_func *= (1 - diag_mask)  # Mask diagonal by multiplying\n",
    "\n",
    "    beta = 100.0  # Controls sharpness of the transition\n",
    "    indicator_func = 1.0 / (1.0 + tensorflow.exp(-beta * (ts_expanded - pairwise_dist[:, :, tensorflow.newaxis])))\n",
    "    \n",
    "    # Sum over all pairs and normalize\n",
    "    #N = tensorflow.cast(tensorflow.shape(dat)[0], tensorflow.float32)  # Number of points\n",
    "    RipleyK_values = (1 / (lambda_ )) * tensorflow.reduce_sum(indicator_func, axis=[0, 1])  # Shape: (samples,)\n",
    "    \n",
    "    return RipleyK_values\n",
    "\n",
    "def RipleyL_vectorized(dat, tpc_radius, min_t, samples=100):\n",
    "    \"\"\"\n",
    "    Compute Ripley's L in a fully vectorized way.\n",
    "    \"\"\"\n",
    "    # Generate range of t values\n",
    "    ts = tensorflow.linspace(min_t, tpc_radius, samples)  # Shape: (samples,)\n",
    "    \n",
    "    # Compute Ripley's K for all t values\n",
    "    K_values = RipleyK_vectorized(dat, ts)\n",
    "    \n",
    "    # Transform to Ripley's L\n",
    "    L_values = tensorflow.sqrt(K_values / tensorflow.constant(np.pi, dtype=tensorflow.float32))\n",
    "    \n",
    "    return L_values, ts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Loss function\n",
    "\n",
    "We want to penalize inhomogeneity (assuming events come from everywhere)\n",
    "\n",
    "Penalize large movements  > 5mm maybe > 2.5mm given the symmetry\n",
    "\n",
    "Prob pretrain on MC? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_func_gen(t, t_min=0.1, t_max=tpc_radius, samples=10, penalize_shift=1.0, max_shift=0.5):\n",
    "    def loss_func(pos_true, pos_pred):\n",
    "        #Rl, ts = RipleyL_vectorized(pos_pred, t_max, t_min, samples)\n",
    "        #loss = tensorflow.square(Rl - ts)\n",
    "        loss = RipleyK_vectorized(pos_pred*tpc_radius, t)\n",
    "        #loss += tensorflow.where(\n",
    "        #        tensorflow.greater(\n",
    "        #            tensorflow.math.abs(tensorflow.math.reduce_euclidean_norm(pos_true, 1, keepdims=True)-tensorflow.math.reduce_euclidean_norm(pos_pred, 1, keepdims=True)), \n",
    "        #            max_shift), \n",
    "        #        penalize_shift, 0.0)\n",
    "        return loss\n",
    "    return loss_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def anode_mesh_perp_wire_dist(positions, tpc_radius):\n",
    "    \"\"\"Taken from Shenyang https://github.com/XENONnT/analysiscode/blob/master/event_patternfit/neural_net_patternfit/Neural_tuned_mc_kr.ipynb\"\"\"\n",
    "    angle = -math.pi/3\n",
    "    xx_rotated = tensorflow.expand_dims((positions[:,0]*tensorflow.math.cos(angle) - positions[:,1]*tensorflow.math.sin(angle)),axis=-1)\n",
    "    yy_rotated = tensorflow.expand_dims((positions[:,0]*tensorflow.math.sin(angle) + positions[:,1]*tensorflow.math.cos(angle)),axis=-1)\n",
    "    dist_anode_mesh = tensorflow.math.minimum(xx_rotated - (xx_rotated//0.5)*0.5, 0.5 - (xx_rotated - (xx_rotated//0.5)*0.5) ) / tpc_radius\n",
    "    # Now the perpendicular wires, in order we have gate0,gate1,anode0,anode1,... this is in rotated frame\n",
    "    wire_pos = tensorflow.constant([[-263, 263, -318, -283, 283, 318]], dtype=tensorflow.float32)\n",
    "    dist_perp = xx_rotated - wire_pos\n",
    "    dist_wall = (tpc_radius - tensorflow.math.reduce_euclidean_norm(positions, 1, keepdims=True) )\n",
    "    \n",
    "    dist_anode_mesh /= 0.5\n",
    "    dist_perp /= tpc_radius\n",
    "    dist_wall /= tpc_radius\n",
    "    positions /= tpc_radius\n",
    "    # return n,10 from n,6 n,1 n,1 and n,2\n",
    "    return tensorflow.concat([dist_perp, dist_anode_mesh, dist_wall, positions], axis=1)\n",
    "\n",
    "class GenerateCoordinates(keras.layers.Layer):\n",
    "    def __init__(self,tpc_radius):\n",
    "        self.tpc_radius = tpc_radius\n",
    "        super().__init__()\n",
    "\n",
    "    def call(self, X):\n",
    "        return anode_mesh_perp_wire_dist(X, self.tpc_radius)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Data\n",
    "\n",
    "We need data formated for anode positions, so we apply the rLCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Livetime 235843.858s\n",
      "Total Datapoints pre filter 1314844\n",
      "Total Datapoints post filter 184657\n"
     ]
    }
   ],
   "source": [
    "# Base data \n",
    "import h5py\n",
    "def get_filtered(path):\n",
    "    with h5py.File(path, 'r') as f:\n",
    "        patterns = f['patterns'][:]\n",
    "        ei = f['event_info'][:]\n",
    "        # run_ids = f[\"run_ids\"][:]\n",
    "        print(\"Total Livetime {}s\".format(f.attrs[\"Livetime_tot_s\"]))\n",
    "        print(\"Total Datapoints pre filter {}\".format(f.attrs[\"OriginalSize\"]))\n",
    "        print(\"Total Datapoints post filter {}\".format(f.attrs[\"CurrentSize\"]))\n",
    "    assert len(patterns) == len(ei)\n",
    "    return patterns, ei\n",
    "\n",
    "filtered_path = '/Code/processed_data/SrCut_z20_0000.hdf5'\n",
    "\n",
    "patterns, ei = get_filtered(filtered_path)\n",
    "positions = jnp.stack([ei['s2_x'], ei['s2_y']],axis = -1)\n",
    "patterns = patterns['s2_area_per_channel'][:, :n_pmts]\n",
    "patterns = tensorflow.convert_to_tensor(patterns, dtype=tensorflow.float32)\n",
    "positions = tensorflow.convert_to_tensor(positions, dtype=tensorflow.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5771/5771 [==============================] - 5s 900us/step\n",
      "Jagged iter 0\n",
      "Epoch 1/20\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 3.6173 - lr: 0.1000\n",
      "Epoch 2/20\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 3.0383 - lr: 0.1000\n",
      "Epoch 3/20\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 2.6332 - lr: 0.1000\n",
      "Epoch 4/20\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 2.3766 - lr: 0.1000\n",
      "Epoch 5/20\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 2.2260 - lr: 0.1000\n",
      "Epoch 6/20\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 2.1434 - lr: 0.1000\n",
      "Epoch 7/20\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 2.1008 - lr: 0.1000\n",
      "Epoch 8/20\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 2.0801 - lr: 0.1000\n",
      "Epoch 9/20\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 2.0708 - lr: 0.1000\n",
      "Epoch 10/20\n",
      "41/46 [=========================>....] - ETA: 0s - loss: 2.0666\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 0.05000000074505806.\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 2.0671 - lr: 0.1000\n",
      "Epoch 11/20\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 2.0636 - lr: 0.0500\n",
      "Epoch 12/20\n",
      "45/46 [============================>.] - ETA: 0s - loss: 2.0566Restoring model weights from the end of the best epoch: 8.\n",
      "\n",
      "Epoch 12: ReduceLROnPlateau reducing learning rate to 0.02500000037252903.\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 2.0566 - lr: 0.0500\n",
      "Epoch 12: early stopping\n",
      "5771/5771 [==============================] - 5s 907us/step\n",
      "Jagged iter 1\n",
      "Epoch 1/20\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 3.6210 - lr: 0.1000\n",
      "Epoch 2/20\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 3.0409 - lr: 0.1000\n",
      "Epoch 3/20\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 2.6352 - lr: 0.1000\n",
      "Epoch 4/20\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 2.3776 - lr: 0.1000\n",
      "Epoch 5/20\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 2.2266 - lr: 0.1000\n",
      "Epoch 6/20\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 2.1436 - lr: 0.1000\n",
      "Epoch 7/20\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 2.1008 - lr: 0.1000\n",
      "Epoch 8/20\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 2.0801 - lr: 0.1000\n",
      "Epoch 9/20\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 2.0708 - lr: 0.1000\n",
      "Epoch 10/20\n",
      "41/46 [=========================>....] - ETA: 0s - loss: 2.0672\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 0.05000000074505806.\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 2.0671 - lr: 0.1000\n",
      "Epoch 11/20\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 2.0636 - lr: 0.0500\n",
      "Epoch 12/20\n",
      "45/46 [============================>.] - ETA: 0s - loss: 2.0566Restoring model weights from the end of the best epoch: 8.\n",
      "\n",
      "Epoch 12: ReduceLROnPlateau reducing learning rate to 0.02500000037252903.\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 2.0566 - lr: 0.0500\n",
      "Epoch 12: early stopping\n",
      "5771/5771 [==============================] - 6s 1ms/step\n",
      "Jagged iter 2\n",
      "Epoch 1/20\n",
      "46/46 [==============================] - 1s 10ms/step - loss: 3.6153 - lr: 0.1000\n",
      "Epoch 2/20\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 3.0365 - lr: 0.1000\n",
      "Epoch 3/20\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 2.6319 - lr: 0.1000\n",
      "Epoch 4/20\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 2.3757 - lr: 0.1000\n",
      "Epoch 5/20\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 2.2255 - lr: 0.1000\n",
      "Epoch 6/20\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 2.1432 - lr: 0.1000\n",
      "Epoch 7/20\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 2.1007 - lr: 0.1000\n",
      "Epoch 8/20\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 2.0801 - lr: 0.1000\n",
      "Epoch 9/20\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 2.0708 - lr: 0.1000\n",
      "Epoch 10/20\n",
      "45/46 [============================>.] - ETA: 0s - loss: 2.0672\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 0.05000000074505806.\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 2.0672 - lr: 0.1000\n",
      "Epoch 11/20\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 2.0637 - lr: 0.0500\n",
      "Epoch 12/20\n",
      "41/46 [=========================>....] - ETA: 0s - loss: 2.0570Restoring model weights from the end of the best epoch: 8.\n",
      "\n",
      "Epoch 12: ReduceLROnPlateau reducing learning rate to 0.02500000037252903.\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 2.0568 - lr: 0.0500\n",
      "Epoch 12: early stopping\n",
      "5771/5771 [==============================] - 7s 1ms/step\n",
      "Jagged iter 3\n",
      "Epoch 1/20\n",
      "46/46 [==============================] - 1s 10ms/step - loss: 3.6214 - lr: 0.1000\n",
      "Epoch 2/20\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 3.0411 - lr: 0.1000\n",
      "Epoch 3/20\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 2.6349 - lr: 0.1000\n",
      "Epoch 4/20\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 2.3774 - lr: 0.1000\n",
      "Epoch 5/20\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 2.2265 - lr: 0.1000\n",
      "Epoch 6/20\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 2.1436 - lr: 0.1000\n",
      "Epoch 7/20\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 2.1008 - lr: 0.1000\n",
      "Epoch 8/20\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 2.0801 - lr: 0.1000\n",
      "Epoch 9/20\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 2.0708 - lr: 0.1000\n",
      "Epoch 10/20\n",
      "41/46 [=========================>....] - ETA: 0s - loss: 2.0670\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 0.05000000074505806.\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 2.0671 - lr: 0.1000\n",
      "Epoch 11/20\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 2.0637 - lr: 0.0500\n",
      "Epoch 12/20\n",
      "45/46 [============================>.] - ETA: 0s - loss: 2.0566Restoring model weights from the end of the best epoch: 8.\n",
      "\n",
      "Epoch 12: ReduceLROnPlateau reducing learning rate to 0.02500000037252903.\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 2.0566 - lr: 0.0500\n",
      "Epoch 12: early stopping\n",
      "5771/5771 [==============================] - 6s 1ms/step\n",
      "Jagged iter 4\n",
      "Epoch 1/20\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 3.6217 - lr: 0.1000\n",
      "Epoch 2/20\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 3.0416 - lr: 0.1000\n",
      "Epoch 3/20\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 2.6353 - lr: 0.1000\n",
      "Epoch 4/20\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 2.3776 - lr: 0.1000\n",
      "Epoch 5/20\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 2.2266 - lr: 0.1000\n",
      "Epoch 6/20\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 2.1439 - lr: 0.1000\n",
      "Epoch 7/20\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 2.1011 - lr: 0.1000\n",
      "Epoch 8/20\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 2.0803 - lr: 0.1000\n",
      "Epoch 9/20\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 2.0709 - lr: 0.1000\n",
      "Epoch 10/20\n",
      "45/46 [============================>.] - ETA: 0s - loss: 2.0672\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 0.05000000074505806.\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 2.0672 - lr: 0.1000\n",
      "Epoch 11/20\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 2.0637 - lr: 0.0500\n",
      "Epoch 12/20\n",
      "41/46 [=========================>....] - ETA: 0s - loss: 2.0566Restoring model weights from the end of the best epoch: 8.\n",
      "\n",
      "Epoch 12: ReduceLROnPlateau reducing learning rate to 0.02500000037252903.\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 2.0567 - lr: 0.0500\n",
      "Epoch 12: early stopping\n",
      "5771/5771 [==============================] - 4s 751us/step\n",
      "Jagged iter 5\n",
      "Epoch 1/20\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 3.6172 - lr: 0.1000\n",
      "Epoch 2/20\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 3.0381 - lr: 0.1000\n",
      "Epoch 3/20\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 2.6331 - lr: 0.1000\n",
      "Epoch 4/20\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 2.3762 - lr: 0.1000\n",
      "Epoch 5/20\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 2.2258 - lr: 0.1000\n",
      "Epoch 6/20\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 2.1432 - lr: 0.1000\n",
      "Epoch 7/20\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 2.1007 - lr: 0.1000\n",
      "Epoch 8/20\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 2.0800 - lr: 0.1000\n",
      "Epoch 9/20\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 2.0708 - lr: 0.1000\n",
      "Epoch 10/20\n",
      "41/46 [=========================>....] - ETA: 0s - loss: 2.0672\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 0.05000000074505806.\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 2.0670 - lr: 0.1000\n",
      "Epoch 11/20\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 2.0636 - lr: 0.0500\n",
      "Epoch 12/20\n",
      "41/46 [=========================>....] - ETA: 0s - loss: 2.0570Restoring model weights from the end of the best epoch: 8.\n",
      "\n",
      "Epoch 12: ReduceLROnPlateau reducing learning rate to 0.02500000037252903.\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 2.0566 - lr: 0.0500\n",
      "Epoch 12: early stopping\n",
      "5771/5771 [==============================] - 6s 969us/step\n",
      "Jagged iter 6\n",
      "Epoch 1/20\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 3.6220 - lr: 0.1000\n",
      "Epoch 2/20\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 3.0419 - lr: 0.1000\n",
      "Epoch 3/20\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 2.6357 - lr: 0.1000\n",
      "Epoch 4/20\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 2.3781 - lr: 0.1000\n",
      "Epoch 5/20\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 2.2270 - lr: 0.1000\n",
      "Epoch 6/20\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 2.1440 - lr: 0.1000\n",
      "Epoch 7/20\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 2.1011 - lr: 0.1000\n",
      "Epoch 8/20\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 2.0802 - lr: 0.1000\n",
      "Epoch 9/20\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 2.0708 - lr: 0.1000\n",
      "Epoch 10/20\n",
      "45/46 [============================>.] - ETA: 0s - loss: 2.0671\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 0.05000000074505806.\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 2.0671 - lr: 0.1000\n",
      "Epoch 11/20\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 2.0637 - lr: 0.0500\n",
      "Epoch 12/20\n",
      "41/46 [=========================>....] - ETA: 0s - loss: 2.0555Restoring model weights from the end of the best epoch: 8.\n",
      "\n",
      "Epoch 12: ReduceLROnPlateau reducing learning rate to 0.02500000037252903.\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 2.0567 - lr: 0.0500\n",
      "Epoch 12: early stopping\n",
      "5771/5771 [==============================] - 5s 813us/step\n",
      "Jagged iter 7\n",
      "Epoch 1/20\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 3.6237 - lr: 0.1000\n",
      "Epoch 2/20\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 3.0432 - lr: 0.1000\n",
      "Epoch 3/20\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 2.6365 - lr: 0.1000\n",
      "Epoch 4/20\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 2.3786 - lr: 0.1000\n",
      "Epoch 5/20\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 2.2273 - lr: 0.1000\n",
      "Epoch 6/20\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 2.1441 - lr: 0.1000\n",
      "Epoch 7/20\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 2.1011 - lr: 0.1000\n",
      "Epoch 8/20\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 2.0802 - lr: 0.1000\n",
      "Epoch 9/20\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 2.0708 - lr: 0.1000\n",
      "Epoch 10/20\n",
      "41/46 [=========================>....] - ETA: 0s - loss: 2.0672\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 0.05000000074505806.\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 2.0671 - lr: 0.1000\n",
      "Epoch 11/20\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 2.0637 - lr: 0.0500\n",
      "Epoch 12/20\n",
      "41/46 [=========================>....] - ETA: 0s - loss: 2.0567Restoring model weights from the end of the best epoch: 8.\n",
      "\n",
      "Epoch 12: ReduceLROnPlateau reducing learning rate to 0.02500000037252903.\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 2.0567 - lr: 0.0500\n",
      "Epoch 12: early stopping\n",
      "5771/5771 [==============================] - 5s 865us/step\n",
      "Jagged iter 8\n",
      "Epoch 1/20\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 3.6205 - lr: 0.1000\n",
      "Epoch 2/20\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 3.0404 - lr: 0.1000\n",
      "Epoch 3/20\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 2.6344 - lr: 0.1000\n",
      "Epoch 4/20\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 2.3771 - lr: 0.1000\n",
      "Epoch 5/20\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 2.2262 - lr: 0.1000\n",
      "Epoch 6/20\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 2.1436 - lr: 0.1000\n",
      "Epoch 7/20\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 2.1008 - lr: 0.1000\n",
      "Epoch 8/20\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 2.0801 - lr: 0.1000\n",
      "Epoch 9/20\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 2.0707 - lr: 0.1000\n",
      "Epoch 10/20\n",
      "45/46 [============================>.] - ETA: 0s - loss: 2.0670\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 0.05000000074505806.\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 2.0670 - lr: 0.1000\n",
      "Epoch 11/20\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 2.0635 - lr: 0.0500\n",
      "Epoch 12/20\n",
      "41/46 [=========================>....] - ETA: 0s - loss: 2.0569Restoring model weights from the end of the best epoch: 8.\n",
      "\n",
      "Epoch 12: ReduceLROnPlateau reducing learning rate to 0.02500000037252903.\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 2.0566 - lr: 0.0500\n",
      "Epoch 12: early stopping\n",
      "5771/5771 [==============================] - 5s 823us/step\n",
      "Jagged iter 9\n",
      "Epoch 1/20\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 3.6166 - lr: 0.1000\n",
      "Epoch 2/20\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 3.0376 - lr: 0.1000\n",
      "Epoch 3/20\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 2.6327 - lr: 0.1000\n",
      "Epoch 4/20\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 2.3760 - lr: 0.1000\n",
      "Epoch 5/20\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 2.2257 - lr: 0.1000\n",
      "Epoch 6/20\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 2.1432 - lr: 0.1000\n",
      "Epoch 7/20\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 2.1006 - lr: 0.1000\n",
      "Epoch 8/20\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 2.0800 - lr: 0.1000\n",
      "Epoch 9/20\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 2.0707 - lr: 0.1000\n",
      "Epoch 10/20\n",
      "41/46 [=========================>....] - ETA: 0s - loss: 2.0672\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 0.05000000074505806.\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 2.0671 - lr: 0.1000\n",
      "Epoch 11/20\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 2.0636 - lr: 0.0500\n",
      "Epoch 12/20\n",
      "41/46 [=========================>....] - ETA: 0s - loss: 2.0567Restoring model weights from the end of the best epoch: 8.\n",
      "\n",
      "Epoch 12: ReduceLROnPlateau reducing learning rate to 0.02500000037252903.\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 2.0566 - lr: 0.0500\n",
      "Epoch 12: early stopping\n",
      "5771/5771 [==============================] - 5s 806us/step\n",
      "Jittering Results\n",
      "Didnt improve : 0.441358843693984%\n",
      "Iteration 0 : 10.413902532804062%\n",
      "Iteration 1 : 10.280682562805636%\n",
      "Iteration 2 : 10.313175238415006%\n",
      "Iteration 3 : 10.014784167402265%\n",
      "Iteration 4 : 9.963337431020758%\n",
      "Iteration 5 : 9.883730375777795%\n",
      "Iteration 6 : 9.850154610981441%\n",
      "Iteration 7 : 9.559345164277552%\n",
      "Iteration 8 : 9.651949289764266%\n",
      "Iteration 9 : 9.627579783057236%\n"
     ]
    }
   ],
   "source": [
    "# Refine Data\n",
    "allow_njit = True\n",
    "keras_loss = logl_loss_generator(pmt_pos_top, keras.backend, not_dead_pmts = not_dead_pmts, cap=float('inf'), allow_njit=allow_njit)\n",
    "if allow_njit:\n",
    "    keras_loss(np.random.random((1, n_pmts)),np.random.random((1, n_pmts)))\n",
    "\n",
    "def get_minimal_LCE_coordinate_model(n_pmts, pmt_positions, n_pmt_pos = 1):\n",
    "    \"\"\"\n",
    "    Generate's PMT Position coordinate, the main one used for now\n",
    "    \"\"\"\n",
    "    conv_pmt_coords = keras.models.Sequential([\n",
    "        keras.layers.RepeatVector(n_pmts, input_shape=(2,)),\n",
    "        GetRadius((n_pmts, n_pmt_pos), keras.backend.variable(pmt_positions)),\n",
    "    ], name='xy_to_pmt_coords')\n",
    "    return conv_pmt_coords\n",
    "\n",
    "def get_RLCEModel(pmt_pos_top,n_pmts, n_groups, group_slices, I0_init, guess,**kwargs):\n",
    "    \"\"\"\n",
    "    Generate Radial LCE Model for fitting\n",
    "    kwargs -> Overflow from old variables\n",
    "    \"\"\"\n",
    "    # Input layer\n",
    "    input_pos = keras.layers.Input(shape=(2))\n",
    "    # Get different coordinates\n",
    "    rho = get_minimal_LCE_coordinate_model(n_pmts = n_pmts, pmt_positions = pmt_pos_top, n_pmt_pos = 1)(input_pos)\n",
    "    # Apply radial lce \n",
    "    m = RadialLCELayer(name='radial_lce',guess= guess, n_groups=n_groups, group_slices=group_slices)(rho)\n",
    "    \n",
    "    # Holds I0 for each PMT\n",
    "    m = I0Layer(n_pmts, init_val= tensorflow.cast(I0_init, tensorflow.float32), name='QE_Layer')(m)\n",
    "    # Normalization Layer \n",
    "    m = NormalizationLayer(name='Normalization')(m)\n",
    "    return keras.models.Model(inputs=input_pos, outputs=m, name=\"Radial_LCE\")\n",
    "\n",
    "npmt_pd = jnp.array([ 1.6263583e+00,  9.4909382e+00, -4.1542644e-05,  7.7986442e-03])\n",
    "I0s = jnp.array([0.004235879983752966, 0.004732195753604174, 0.00466069346293807, 0.004735287744551897, 0.004843718837946653, 0.0036560967564582825, 0.004791035782545805, 0.004351622425019741, 0.004277762491255999, 0.003982590511441231, 0.0036782578099519014, 0.004139808006584644, 0.003963056020438671, 0.004514740779995918, 0.0048719532787799835, 0.003294900059700012, 0.004271252546459436, 0.004213831853121519, 0.003675800282508135, 0.003923185635358095, 0.004053255543112755, 0.003687959862872958, 0.003949134610593319, 0.0039024786092340946, 0.003996435087174177, 0.0045274775475263596, 0.00314155500382185, 0.004692835733294487, 0.0017648281063884497, 0.003712652949616313, 0.0041131614707410336, 0.0039019035175442696, 0.0038532845210283995, 0.003392500802874565, 0.003896859474480152, 0.003918000962585211, 0.004090684931725264, 0.004209689795970917, 0.004247184377163649, 0.0050539360381662846, 0.004631971474736929, 0.0039286985993385315, 0.003714651335030794, 0.003667386481538415, 0.004035579971969128, 0.003805769607424736, 0.003694107523187995, 0.0038736388087272644, 0.004259074572473764, 0.0036939873825758696, 0.004129675682634115, 0.004015707410871983, 0.0042891716584563255, 0.005394953768700361, 0.0049893721006810665, 0.004004128277301788, 0.0036364563275128603, 0.0037317327223718166, 0.0037540518678724766, 0.004226438235491514, 0.003232576884329319, 0.004007598850876093, 0.003992777783423662, 0.00392210902646184, 0.003881508018821478, 0.0037358906120061874, 0.0034426788333803415, 0.004593325313180685, 0.004476095549762249, 0.004509852733463049, 0.00398320984095335, 0.0038602910935878754, 0.0038099722005426884, 0.0035987216979265213, 0.004056497476994991, 0.003779412480071187, 0.003909052349627018, 0.003880080534145236, 0.0037605699617415667, 0.0040501500479876995, 0.003605334088206291, 0.0037680561654269695, 0.003598541719838977, 0.0036737960763275623, 0.004982828162610531, 0.004214885178953409, 0.00394063163548708, 0.00341225927695632, 0.0037580307107418776, 0.003971084486693144, 0.004098842386156321, 0.003986097406595945, 0.004088491201400757, 0.004092586226761341, 0.003926937002688646, 0.004298168700188398, 0.004080779384821653, 0.003439064836129546, 0.003919169306755066, 0.0038085675332695246, 0.004447304643690586, 0.004079096484929323, 0.004161607474088669, 0.0035417259205132723, 0.004211203660815954, 0.003966113086789846, 0.004028564319014549, 0.0038740618620067835, 0.001677889609709382, 0.004096835386008024, 0.004131924360990524, 0.004420299082994461, 0.0038429093547165394, 0.003977605141699314, 0.004036216530948877, 0.0040572527796030045, 0.0034212754108011723, 0.004304381553083658, 0.004800576251000166, 0.003856675000861287, 0.0037602523807436228, 0.0017064257990568876, 0.003873104928061366, 0.003873534733429551, 0.003949661739170551, 0.004201176576316357, 0.0039010290056467056, 0.003948729019612074, 0.004660475999116898, 0.003974431660026312, 0.004105430096387863, 0.003848433494567871, 0.0039720600470900536, 0.004035878926515579, 0.005145389586687088, 0.004992862232029438, 0.004138431046158075, 0.0038946077693253756, 0.0037270355969667435, 0.003934138920158148, 0.004055908881127834, 0.004122219048440456, 0.00413487758487463, 0.004022635985165834, 0.0016843285411596298, 0.0039187888614833355, 0.004069270566105843, 0.004088082350790501, 0.0038323558401316404, 0.003933216445147991, 0.0041273306123912334, 0.00391603447496891, 0.003779760794714093, 0.0038304287008941174, 0.003971415106207132, 0.003770100185647607, 0.0016588973812758923, 0.003752924269065261, 0.0038747305516153574, 0.004287980031222105, 0.004396375268697739, 0.004206789191812277, 0.004279491025954485, 0.0038022841326892376, 0.0016486833337694407, 0.003921540919691324, 0.00393262505531311, 0.0036610763054341078, 0.005085785873234272, 0.00415172940120101, 0.0035354148130863905, 0.004259951412677765, 0.0040181828662753105, 0.003962669055908918, 0.004295114893466234, 0.004105494357645512, 0.00389536889269948, 0.0016863005002960563, 0.004174431320279837, 0.0035444151144474745, 0.0036017769016325474, 0.0038141191471368074, 0.0036408151499927044, 0.004543586168438196, 0.004658541176468134, 0.0037312631029635668, 0.0039724307134747505, 0.0036708605475723743, 0.003768590744584799, 0.003977857064455748, 0.003958065528422594, 0.00391046516597271, 0.004013632424175739, 0.004107540939003229, 0.004054495599120855, 0.003917690832167864, 0.0037647797726094723, 0.003846473526209593, 0.005038938019424677, 0.005282974801957607, 0.003871278138831258, 0.004100256133824587, 0.003908885642886162, 0.004233073443174362, 0.003930484410375357, 0.0037340044509619474, 0.0039030059706419706, 0.0037919552996754646, 0.00414620153605938, 0.003158329986035824, 0.0034461934119462967, 0.004152875859290361, 0.005023165140300989, 0.005013687536120415, 0.004022886045277119, 0.0038592142518609762, 0.003895816160365939, 0.004128684755414724, 0.0037005161866545677, 0.00402730843052268, 0.003608384868130088, 0.0027551278471946716, 0.004201559349894524, 0.0035797676537185907, 0.003656105836853385, 0.004713757894933224, 0.003788005094975233, 0.004254915285855532, 0.0041169957257807255, 0.004034965764731169, 0.0036038379184901714, 0.003389847930520773, 0.003905089572072029, 0.0038801683112978935, 0.0035936341155320406, 0.004151667468249798, 0.0040817963890731335, 0.004347157198935747, 0.004485994577407837, 0.00449867220595479, 0.004051011521369219, 0.0041497559286653996, 0.00477506872266531, 0.004078592173755169, 0.004272844642400742, 0.0041829245164990425, 0.005090698599815369, 0.003888470819219947, 0.004583718255162239, 0.005087660159915686, 0.0047324057668447495, 0.0045408145524561405, 0.0037471496034413576])\n",
    "\n",
    "radial_lce = get_RLCEModel(pmt_pos_top[:,2:],n_pmts=n_pmts, n_groups=1, group_slices=np.arange(n_pmts), \n",
    "                        I0_init=I0_init_from_ULCE, guess = npmt_pd.tolist())\n",
    "radial_lce.layers[-2].set_weights([I0s])\n",
    "\n",
    "def get_position_updater(positions, n_to_update, model,input_len = 1, embeddings_constraint=[]):\n",
    "    \"\"\"\n",
    "    Returns encoder model to get patterns from positions\n",
    "    \"\"\"\n",
    "    input = keras.layers.Input(shape=(input_len,))\n",
    "    if embeddings_constraint != []:\n",
    "        m = keras.layers.Embedding(n_to_update, 2, input_length = input_len, name='embed_pos',embeddings_constraint=embeddings_constraint)(input)\n",
    "    else: \n",
    "        m = keras.layers.Embedding(n_to_update, 2, input_length = input_len, name='embed_pos')(input)\n",
    "    m = keras.layers.Flatten()(m)\n",
    "    m = model(m)\n",
    "    pos_updater = keras.models.Model(inputs= input, outputs=m, name=\"Position_Updater\")\n",
    "    pos_updater.layers[-1].trainable = False\n",
    "    pos_updater.get_layer('embed_pos').set_weights([positions[:n_to_update]])\n",
    "\n",
    "    return pos_updater\n",
    "def train_and_update_pos(positions, patterns, model, n_iter = 5, verbose=0, hist_pos=False, keras_loss=keras_loss):\n",
    "    early_stopping = tensorflow.keras.callbacks.EarlyStopping(\n",
    "        monitor='loss',\n",
    "        restore_best_weights=True,\n",
    "        patience=4,    \n",
    "        min_delta=0.05,\n",
    "        mode='min',    \n",
    "        verbose=1      \n",
    "    )\n",
    "    reduce_lr = tensorflow.keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='loss', \n",
    "        mode = 'min',\n",
    "        factor=0.5,     \n",
    "        patience=2,     \n",
    "        min_delta=0.05, \n",
    "        min_lr=1e-3,    \n",
    "        verbose=1       \n",
    "    )\n",
    "    n_tot = len(positions)\n",
    "    pos_updater = get_position_updater(positions, n_tot, model ,input_len = 1)\n",
    "    # Inspecting the degree of change\n",
    "    if hist_pos:\n",
    "        trained_pos = []\n",
    "\n",
    "    new_pos = tensorflow.identity(positions)\n",
    "    loss    = keras_loss(patterns, model.predict(positions))\n",
    "    improved= jnp.zeros(positions.shape[0])\n",
    "    improved[:] = -1\n",
    "    for i in range(n_iter):\n",
    "        print(\"Jagged iter {}\".format(i))\n",
    "        # tensorflow_probability requires tensorflow >= 2.16 we have == 2.15\n",
    "        # Random dist gives max deviation root{2}*pmt_radius (~70% of diameter)\n",
    "        noise_pos = tensorflow.identity(positions) + jnp.random.rand(*new_pos.numpy().shape) * pmt_radius\n",
    "        # Update and fit encoder model positions\n",
    "        pos_updater.get_layer('embed_pos').set_weights([noise_pos])\n",
    "        # Compile and fit\n",
    "        pos_updater.compile(loss= keras_loss, optimizer = keras.optimizers.Adam(learning_rate=1e-1))\n",
    "        pos_updater.fit(x = jnp.arange(n_tot), y = patterns, \n",
    "                        epochs = 20, batch_size = 2**12,\n",
    "                        callbacks=[early_stopping,reduce_lr], verbose = verbose) \n",
    "        noise_pos = pos_updater.get_layer('embed_pos').get_weights()[0]\n",
    "        # Check result of new pos\n",
    "        jittered_loss = keras_loss(patterns, model.predict(noise_pos, verbose=verbose))\n",
    "\n",
    "        better = jittered_loss < loss\n",
    "        loss =    tensorflow.where(better, jittered_loss, loss) \n",
    "        new_pos = tensorflow.where(tensorflow.expand_dims(better, axis=-1), noise_pos, new_pos) \n",
    "        improved[better.numpy()] = i\n",
    "        if hist_pos:\n",
    "            trained_pos.append(noise_pos)\n",
    "\n",
    "    print(\"Jittering Results\")\n",
    "    print(\"Didnt improve : {}%\".format((improved == -1).mean()*100))\n",
    "    for i in range(n_iter):\n",
    "        print(\"Iteration {} : {}%\".format(i, (improved == i).mean()*100))\n",
    "    if not hist_pos:\n",
    "        return new_pos\n",
    "    else:\n",
    "        return new_pos, trained_pos\n",
    "        \n",
    "new_pos=train_and_update_pos(positions, patterns, radial_lce, n_iter = 10, verbose=1, hist_pos=False).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Back to Anode to Gate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Multi Layer MLP\n",
    "\n",
    "\n",
    "Using tutorial code as starting point\n",
    "\"\"\"\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import grad, jit, vmap\n",
    "from jax import random\n",
    "\n",
    "n_in = 2\n",
    "\n",
    "# ------------------------------- Init Weights ---------------------------------\n",
    "def random_layer_params(m, n, key, scale):\n",
    "    # Random init of weights\n",
    "    w_key, b_key = random.split(key)\n",
    "    return scale * random.normal(w_key, (n,m)), scale*random.normal(b_key, (n,))\n",
    "\n",
    "def init_network_params(sizes, key, scale):\n",
    "    keys = random.split(key,len(sizes))\n",
    "    return [random_layer_params(m,n,k, scale) for m,n,k in zip(sizes[:-1],sizes[1:],keys)]\n",
    "\n",
    "# ------------------------------- Training func --------------------------------\n",
    "from jax.scipy.special import logsumexp\n",
    "\n",
    "def relu(x): # TODO Change\n",
    "  return jnp.maximum(0, x)\n",
    "\n",
    "def predict(params, image):\n",
    "    activations = image\n",
    "\n",
    "    # For weight, bias in layers\n",
    "    for w,b in params[:-1]:\n",
    "        outputs = jnp.dot(w, activations) + b\n",
    "        activations = relu(outputs)\n",
    "    # Get last layer\n",
    "    final_w, final_b = params[-1]\n",
    "    return jnp.dot(final_w, activations) + final_b\n",
    "\n",
    "batched_predict = vmap(predict, in_axes=(None, 0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model params\n",
    "\n",
    "layer_sizes = [2, 2**8, 2**12, 2**8, 2]\n",
    "step_size = 0.01\n",
    "num_epochs = 10\n",
    "batch_size = 128\n",
    "n_targets = 10\n",
    "params = init_network_params(layer_sizes, random.key(0),9e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Def function to actually compute per item loss\n",
    "from jax import lax\n",
    "def indicator_function_lazy(dat, t):\n",
    "    n = dat.shape[0]\n",
    "    def compute_row(i, acc):\n",
    "        row = lax.dynamic_index_in_dim(dat, i, axis=0, keepdims=False) \n",
    "        distances = jnp.linalg.norm(row - dat, axis=1)\n",
    "        #indicator_row = (distances < t).at[i].set(0.0)  # Set diagonal to 0\n",
    "        indicator_row = (1 / (1 + jnp.exp(100 * (distances - t)))).at[i].set(0.0)\n",
    "        return acc + jnp.sum(indicator_row)\n",
    "    result = lax.fori_loop(0, n, compute_row, 0.0)\n",
    "    return result\n",
    "@jit\n",
    "def RipleyK(dat, t, lambda_):\n",
    "    #pairwise_dist = jnp.linalg.norm(dat[:, jnp.newaxis, :] - dat[jnp.newaxis, :, :], axis=2)\n",
    "    #diagonal_mask = ~jnp.eye(pairwise_dist.shape[0], dtype=bool)  # Mask for i != j\n",
    "    #indicator_func = (pairwise_dist < t) * diagonal_mask\n",
    "    return (1/(lambda_ * dat.shape[0])) * indicator_function_lazy(dat, t)\n",
    "\n",
    "def RipleyL(dat, tpc_radius, min_t, lambda_,samples=10, ):\n",
    "    ts = jnp.linspace(min_t, tpc_radius, samples)\n",
    "    RipleyK_vmap = jax.vmap(lambda t: jnp.sqrt(RipleyK(dat, t, lambda_) / jnp.pi))\n",
    "    Ls = RipleyK_vmap(ts)\n",
    "    return Ls, ts\n",
    "@jit\n",
    "def accuracy(params, x_pos, lambda_,min_t, tpc_radius, y_pos = None):\n",
    "    return RipleyL(batched_predict(params, x_pos), tpc_radius=tpc_radius, min_t=min_t,lambda_ = lambda_, samples = 4) # 1 mm -> 5mm grid spacing\n",
    "\n",
    "# Def loss function -> Returns single item\n",
    "@jit\n",
    "def loss(params, x_pos,lambda_,t, y_pos = None):\n",
    "    tmp = RipleyK(batched_predict(params, x_pos), t=t, lambda_ = lambda_)\n",
    "    #print(tmp)\n",
    "    return tmp\n",
    "\n",
    "# Def function to update weights\n",
    "@jit\n",
    "def update(params, x, y,  lambda_, step_size, t):\n",
    "    grads = grad(loss)(params, x, lambda_, t)\n",
    "    #print(grads)\n",
    "    return [(w - step_size * dw, b - step_size * db)\n",
    "            for (w, b), (dw, db) in zip(params, grads)]\n",
    "\n",
    "def create_mini_batches(x, y, batch_size, key):\n",
    "    num_samples = len(x)\n",
    "    key, subkey = jax.random.split(key)\n",
    "    indices = jax.random.permutation(subkey, num_samples)\n",
    "    x_shuffled = x[indices]\n",
    "    y_shuffled = y[indices]\n",
    "    \n",
    "    for start in range(0, num_samples, batch_size):\n",
    "        end = min(start + batch_size, num_samples)\n",
    "        yield x_shuffled[start:end], y_shuffled[start:end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get indices\n",
    "shuffle = np.random.permutation(len(new_pos))\n",
    "train_indcs, test__indcs = shuffle[0: int(len(new_pos) *0.8)], shuffle[int(len(new_pos) *0.8): len(new_pos)]\n",
    "unscale = (new_pos.mean(), new_pos.std())\n",
    "new_pos_scale = (new_pos - unscale[0]) / unscale[1]\n",
    "x_train, y_train = new_pos_scale[train_indcs], new_pos_scale[train_indcs]\n",
    "x_test,  y_test  = new_pos_scale[test__indcs], new_pos_scale[test__indcs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 in 9.47 sec\n",
      "Test set Ripley L var: nan; mean: nan\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[146], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m key, subkey \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39msplit(key)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x, y \u001b[38;5;129;01min\u001b[39;00m create_mini_batches(x_train, y_train, batch_size, subkey):\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;66;03m# This applies loss func and updates weights\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m     params \u001b[38;5;241m=\u001b[39m \u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlambda_\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtpc_radius\u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43munscale\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstep_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstep_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43munscale\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m epoch_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m in \u001b[39m\u001b[38;5;132;01m{:0.2f}\u001b[39;00m\u001b[38;5;124m sec\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(epoch, epoch_time))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# TODO Peanalize large shifts in loss\n",
    "key = jax.random.PRNGKey(42)\n",
    "\n",
    "step_size = 0.1\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    start_time = time.time()\n",
    "    # Iterate batches and do thing\n",
    "    key, subkey = jax.random.split(key)\n",
    "    for x, y in create_mini_batches(x_train, y_train, batch_size, subkey):\n",
    "        # This applies loss func and updates weights\n",
    "        params = update(params, x, y, lambda_=jnp.pi * (tpc_radius/unscale[1])**2, step_size=step_size, t = 2/unscale[1])\n",
    "    epoch_time = time.time() - start_time\n",
    "\n",
    "    print(\"Epoch {} in {:0.2f} sec\".format(epoch, epoch_time))\n",
    "    #Ls, ts = accuracy(params, x_train)\n",
    "    #print(\"Training set Ripley L var: {}; mean: {}\".format(jnp.std(Ls-ts), jnp.mean(Ls-ts)))\n",
    "    Ls, ts = accuracy(params, x_test, lambda_=jnp.pi * (tpc_radius/unscale[1])**2, min_t = (0.1/unscale[1]), tpc_radius=tpc_radius/unscale[1])\n",
    "    print(\"Test set Ripley L var: {}; mean: {}\".format(jnp.std(Ls-ts), jnp.mean(Ls-ts)))\n",
    "    #Ls, ts = accuracy(params, new_pos)\n",
    "    #print(\"Full set Ripley L var: {}; mean: {}\".format(jnp.std(Ls-ts), jnp.mean(Ls-ts)))\n",
    "    #gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradients: [(Array([[nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan],\n",
      "       [nan, nan]], dtype=float32), Array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan], dtype=float32)), (Array([[nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       ...,\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan]], dtype=float32), Array([nan, nan, nan, ..., nan, nan, nan], dtype=float32)), (Array([[nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       ...,\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan]], dtype=float32), Array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan], dtype=float32)), (Array([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan]], dtype=float32), Array([nan, nan], dtype=float32))]\n"
     ]
    }
   ],
   "source": [
    "grads = grad(loss)(params, x, jnp.pi * (tpc_radius/unscale[1])**2, t=1)\n",
    "print(f\"Gradients: {grads}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((256, 2), (256,), (4096, 256), (4096,))"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grads[0][0].shape, grads[0][1].shape, grads[1][0].shape, grads[1][1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning JAX - Above rewritten version of below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Really basic model to understand what im doing\n",
    "\"\"\"\n",
    "Single Layer MLP\n",
    "\n",
    "\n",
    "key corresponds to random initializer for reproducibility, random.split(key) produces subkeys\n",
    "\n",
    "\"\"\"\n",
    "import jax.numpy as jnp\n",
    "from jax import grad, jit, vmap\n",
    "from jax import random\n",
    "\n",
    "\n",
    "# ------------------------------- Init Weights ---------------------------------\n",
    "def random_layer_params(m, n, key, scale=1e-2):\n",
    "    # Random init of weights\n",
    "    w_key, b_key = random.split(key)\n",
    "    return scale * random.normal(w_key, (n,m)), scale*random.normal(b_key, (n,))\n",
    "\n",
    "def init_network_params(sizes, key):\n",
    "    keys = random.split(key,len(sizes))\n",
    "    return [random_layer_params(m,n,k) for m,n,k in zip(sizes[:-1],sizes[1:],keys)]\n",
    "\n",
    "# ------------------------------- Training func --------------------------------\n",
    "from jax.scipy.special import logsumexp\n",
    "\n",
    "def relu(x):\n",
    "  return jnp.maximum(0, x)\n",
    "\n",
    "def predict(params, image):\n",
    "    activations = image\n",
    "\n",
    "    # For weight, bias in layers\n",
    "    for w,b in params[:-1]:\n",
    "        outputs = jnp.dot(w, activations) + b\n",
    "        activations = relu(outputs)\n",
    "    # Get last layer\n",
    "    final_w, final_b = params[-1]\n",
    "    # Logits are raw unnormalized output params before activation function \n",
    "    logits = jnp.dot(final_w, activations) + final_b\n",
    "    return logits - logsumexp(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model params\n",
    "\n",
    "layer_sizes = [784, 512, 512, 10]\n",
    "step_size = 0.01\n",
    "num_epochs = 10\n",
    "batch_size = 128\n",
    "n_targets = 10\n",
    "params = init_network_params(layer_sizes, random.key(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10,)\n"
     ]
    }
   ],
   "source": [
    "random_flattened_image = random.normal(random.key(1), (28 * 28,))\n",
    "preds = predict(params, random_flattened_image)\n",
    "print(preds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 10)\n"
     ]
    }
   ],
   "source": [
    "# Apply predict that works on single item to batches\n",
    "batched_predict = vmap(predict, in_axes=(None, 0))\n",
    "\n",
    "random_flattened_images = random.normal(random.key(1), (10, 28 * 28))\n",
    "batched_preds = batched_predict(params, random_flattened_images)\n",
    "print(batched_preds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(x, k, dtype=jnp.float32):\n",
    "    \"\"\"Create a one-hot encoding of x of size k.\"\"\"\n",
    "    return jnp.array(x[:, None] == jnp.arange(k), dtype)\n",
    "\n",
    "# Def function to actually compute per item loss\n",
    "def accuracy(params, images, targets):\n",
    "    target_class = jnp.argmax(targets, axis=1)\n",
    "    predicted_class = jnp.argmax(batched_predict(params, images), axis=1)\n",
    "    return jnp.mean(predicted_class == target_class)\n",
    "\n",
    "# Def loss function -> Returns single item\n",
    "def loss(params, images, targets):\n",
    "    preds = batched_predict(params, images)\n",
    "    return -jnp.mean(preds * targets)\n",
    "\n",
    "# Def function to update weights\n",
    "@jit\n",
    "def update(params, x, y):\n",
    "    grads = grad(loss)(params, x, y)\n",
    "    return [(w - step_size * dw, b - step_size * db)\n",
    "            for (w, b), (dw, db) in zip(params, grads)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/XENONnT/anaconda/envs/XENONnT_el9.2024.10.4/lib/python3.9/pty.py:85: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  pid, fd = os.forkpty()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow_datasets\n",
      "  Downloading tensorflow_datasets-4.9.3-py3-none-any.whl.metadata (9.3 kB)\n",
      "Requirement already satisfied: absl-py in /opt/XENONnT/anaconda/envs/XENONnT_el9.2024.10.4/lib/python3.9/site-packages (from tensorflow_datasets) (2.1.0)\n",
      "Collecting array-record (from tensorflow_datasets)\n",
      "  Downloading array_record-0.5.1-1-cp39-cp39-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (699 bytes)\n",
      "Requirement already satisfied: click in /opt/XENONnT/anaconda/envs/XENONnT_el9.2024.10.4/lib/python3.9/site-packages (from tensorflow_datasets) (8.1.7)\n",
      "Requirement already satisfied: dm-tree in /opt/XENONnT/anaconda/envs/XENONnT_el9.2024.10.4/lib/python3.9/site-packages (from tensorflow_datasets) (0.1.8)\n",
      "Requirement already satisfied: etils>=0.9.0 in /opt/XENONnT/anaconda/envs/XENONnT_el9.2024.10.4/lib/python3.9/site-packages (from etils[enp,epath,etree]>=0.9.0->tensorflow_datasets) (1.5.2)\n",
      "Requirement already satisfied: numpy in /opt/XENONnT/anaconda/envs/XENONnT_el9.2024.10.4/lib/python3.9/site-packages (from tensorflow_datasets) (1.26.4)\n",
      "Collecting promise (from tensorflow_datasets)\n",
      "  Downloading promise-2.3.tar.gz (19 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: protobuf>=3.20 in /opt/XENONnT/anaconda/envs/XENONnT_el9.2024.10.4/lib/python3.9/site-packages (from tensorflow_datasets) (4.25.5)\n",
      "Requirement already satisfied: psutil in /opt/XENONnT/anaconda/envs/XENONnT_el9.2024.10.4/lib/python3.9/site-packages (from tensorflow_datasets) (5.9.8)\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/XENONnT/anaconda/envs/XENONnT_el9.2024.10.4/lib/python3.9/site-packages (from tensorflow_datasets) (2.31.0)\n",
      "Collecting tensorflow-metadata (from tensorflow_datasets)\n",
      "  Downloading tensorflow_metadata-1.16.1-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: termcolor in /opt/XENONnT/anaconda/envs/XENONnT_el9.2024.10.4/lib/python3.9/site-packages (from tensorflow_datasets) (2.5.0)\n",
      "Collecting toml (from tensorflow_datasets)\n",
      "  Downloading toml-0.10.2-py2.py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: tqdm in /opt/XENONnT/anaconda/envs/XENONnT_el9.2024.10.4/lib/python3.9/site-packages (from tensorflow_datasets) (4.66.5)\n",
      "Requirement already satisfied: wrapt in /opt/XENONnT/anaconda/envs/XENONnT_el9.2024.10.4/lib/python3.9/site-packages (from tensorflow_datasets) (1.14.1)\n",
      "Requirement already satisfied: fsspec in /opt/XENONnT/anaconda/envs/XENONnT_el9.2024.10.4/lib/python3.9/site-packages (from etils[enp,epath,etree]>=0.9.0->tensorflow_datasets) (2022.11.0)\n",
      "Requirement already satisfied: importlib_resources in /opt/XENONnT/anaconda/envs/XENONnT_el9.2024.10.4/lib/python3.9/site-packages (from etils[enp,epath,etree]>=0.9.0->tensorflow_datasets) (6.4.5)\n",
      "Requirement already satisfied: typing_extensions in /opt/XENONnT/anaconda/envs/XENONnT_el9.2024.10.4/lib/python3.9/site-packages (from etils[enp,epath,etree]>=0.9.0->tensorflow_datasets) (4.12.2)\n",
      "Requirement already satisfied: zipp in /opt/XENONnT/anaconda/envs/XENONnT_el9.2024.10.4/lib/python3.9/site-packages (from etils[enp,epath,etree]>=0.9.0->tensorflow_datasets) (3.20.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/XENONnT/anaconda/envs/XENONnT_el9.2024.10.4/lib/python3.9/site-packages (from requests>=2.19.0->tensorflow_datasets) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/XENONnT/anaconda/envs/XENONnT_el9.2024.10.4/lib/python3.9/site-packages (from requests>=2.19.0->tensorflow_datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/XENONnT/anaconda/envs/XENONnT_el9.2024.10.4/lib/python3.9/site-packages (from requests>=2.19.0->tensorflow_datasets) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/XENONnT/anaconda/envs/XENONnT_el9.2024.10.4/lib/python3.9/site-packages (from requests>=2.19.0->tensorflow_datasets) (2024.8.30)\n",
      "Requirement already satisfied: six in /opt/XENONnT/anaconda/envs/XENONnT_el9.2024.10.4/lib/python3.9/site-packages (from promise->tensorflow_datasets) (1.16.0)\n",
      "Collecting protobuf>=3.20 (from tensorflow_datasets)\n",
      "  Downloading protobuf-3.20.3-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.whl.metadata (679 bytes)\n",
      "Downloading tensorflow_datasets-4.9.3-py3-none-any.whl (5.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.0/5.0 MB\u001b[0m \u001b[31m51.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading array_record-0.5.1-1-cp39-cp39-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (2.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m70.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tensorflow_metadata-1.16.1-py3-none-any.whl (28 kB)\n",
      "Downloading protobuf-3.20.3-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m50.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading toml-0.10.2-py2.py3-none-any.whl (16 kB)\n",
      "Building wheels for collected packages: promise\n",
      "  Building wheel for promise (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for promise: filename=promise-2.3-py3-none-any.whl size=21483 sha256=80c670f1542e77d8c14943449d0aa18a8861e2a1b7cedb937dd8dc1bd0810572\n",
      "  Stored in directory: /root/.cache/pip/wheels/e1/e8/83/ddea66100678d139b14bc87692ece57c6a2a937956d2532608\n",
      "Successfully built promise\n",
      "Installing collected packages: toml, protobuf, promise, tensorflow-metadata, array-record, tensorflow_datasets\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 4.25.5\n",
      "    Uninstalling protobuf-4.25.5:\n",
      "      Successfully uninstalled protobuf-4.25.5\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow 2.15.0.post1 requires ml-dtypes~=0.2.0, but you have ml-dtypes 0.5.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed array-record-0.5.1 promise-2.3 protobuf-3.20.3 tensorflow-metadata-1.16.1 tensorflow_datasets-4.9.3 toml-0.10.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install tensorflow_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (60000, 784) (60000, 10)\n",
      "Test: (10000, 784) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "# Dataload with tf for images\n",
    "import tensorflow as tf\n",
    "# Ensure TF does not see GPU and grab all GPU memory.\n",
    "tf.config.set_visible_devices([], device_type='GPU')\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "data_dir = '/tmp/tfds'\n",
    "\n",
    "# Fetch full datasets for evaluation\n",
    "# tfds.load returns tf.Tensors (or tf.data.Datasets if batch_size != -1)\n",
    "# You can convert them to NumPy arrays (or iterables of NumPy arrays) with tfds.dataset_as_numpy\n",
    "mnist_data, info = tfds.load(name=\"mnist\", batch_size=-1, data_dir=data_dir, with_info=True)\n",
    "mnist_data = tfds.as_numpy(mnist_data)\n",
    "train_data, test_data = mnist_data['train'], mnist_data['test']\n",
    "num_labels = info.features['label'].num_classes\n",
    "h, w, c = info.features['image'].shape\n",
    "num_pixels = h * w * c\n",
    "\n",
    "# Full train set\n",
    "train_images, train_labels = train_data['image'], train_data['label']\n",
    "train_images = jnp.reshape(train_images, (len(train_images), num_pixels))\n",
    "train_labels = one_hot(train_labels, num_labels)\n",
    "\n",
    "# Full test set\n",
    "test_images, test_labels = test_data['image'], test_data['label']\n",
    "test_images = jnp.reshape(test_images, (len(test_images), num_pixels))\n",
    "test_labels = one_hot(test_labels, num_labels)\n",
    "\n",
    "print('Train:', train_images.shape, train_labels.shape)\n",
    "print('Test:', test_images.shape, test_labels.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 in 4.15 sec\n",
      "Training set accuracy 0.9253333210945129\n",
      "Test set accuracy 0.9266999959945679\n",
      "Epoch 1 in 3.47 sec\n",
      "Training set accuracy 0.9427833557128906\n",
      "Test set accuracy 0.9412999749183655\n",
      "Epoch 2 in 2.64 sec\n",
      "Training set accuracy 0.9532666802406311\n",
      "Test set accuracy 0.9511999487876892\n",
      "Epoch 3 in 2.85 sec\n",
      "Training set accuracy 0.9598833322525024\n",
      "Test set accuracy 0.9557999968528748\n",
      "Epoch 4 in 3.39 sec\n",
      "Training set accuracy 0.9651333689689636\n",
      "Test set accuracy 0.9599999785423279\n",
      "Epoch 5 in 2.89 sec\n",
      "Training set accuracy 0.9691500067710876\n",
      "Test set accuracy 0.9629999995231628\n",
      "Epoch 6 in 3.70 sec\n",
      "Training set accuracy 0.9725666642189026\n",
      "Test set accuracy 0.9648999571800232\n",
      "Epoch 7 in 3.46 sec\n",
      "Training set accuracy 0.9754166603088379\n",
      "Test set accuracy 0.9666999578475952\n",
      "Epoch 8 in 3.19 sec\n",
      "Training set accuracy 0.9780833721160889\n",
      "Test set accuracy 0.9680999517440796\n",
      "Epoch 9 in 3.53 sec\n",
      "Training set accuracy 0.9803333282470703\n",
      "Test set accuracy 0.9691999554634094\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Batch training data\n",
    "def get_train_batches():\n",
    "    # as_supervised=True gives us the (image, label) as a tuple instead of a dict\n",
    "    ds = tfds.load(name='mnist', split='train', as_supervised=True, data_dir=data_dir)\n",
    "    # You can build up an arbitrary tf.data input pipeline\n",
    "    ds = ds.batch(batch_size).prefetch(1)\n",
    "    # tfds.dataset_as_numpy converts the tf.data.Dataset into an iterable of NumPy arrays\n",
    "    return tfds.as_numpy(ds)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    start_time = time.time()\n",
    "    # Iterate batches and do thing\n",
    "    for x, y in get_train_batches():\n",
    "        x = jnp.reshape(x, (len(x), num_pixels))\n",
    "        y = one_hot(y, num_labels)\n",
    "        # This applies loss func and updates weights\n",
    "        params = update(params, x, y)\n",
    "    epoch_time = time.time() - start_time\n",
    "\n",
    "    # Get acc\n",
    "    train_acc = accuracy(params, train_images, train_labels)\n",
    "    test_acc = accuracy(params, test_images, test_labels)\n",
    "    print(\"Epoch {} in {:0.2f} sec\".format(epoch, epoch_time))\n",
    "    print(\"Training set accuracy {}\".format(train_acc))\n",
    "    print(\"Test set accuracy {}\".format(test_acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
